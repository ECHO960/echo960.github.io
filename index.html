<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Chunhui Liu</title>

    <meta name="author" content="Chunhui Liu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Chunhui Liu
                </p>
                <p>I'm a Senior Machine Learning Engineer at TikTok's Video Search Team, where I serve as the tech lead for relevance and pretraining. I work with a group of talented individuals to build the world's largest short video search engine with tens of billions videos, and serving billions of users daily. My team focuses on developing BERT/LLM models as part of the search engine to ensure that the results are safe and relevant, utilizing advanced techniques in CV/NLP/Multimodal learning and pretraining.
                </p>
                <p>
                  I was previously an Applied Scientist at Amazon AI, where I conducted cutting-edge research and developed real-world applications for video and action understanding from cameras. I obtained my Master's degree in Computer Vision from CMU and my Bachelor's degree in Computer Science, Summa Cum Laude, from Peking University, under the supervision of Prof. Jiaying Liu.
                </p>
                <p style="text-align:center">
                  <a href="mailto:chunhuiliu960@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=03DnyVsAAAAJ&hl=zh-CN">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/echo960/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/lch.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/lch.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p>
                <span class="highlight">We are hiring!</span>
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research and Selected Publications</h2>
                <p>
                  Recently, I have not been an active researcher on public topics. However, I remain deeply interested in deep learning, computer vision, multi-modality learning, and video understanding. I am passionate about building machine learning models that understand the physical world in a way similar to humans.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

<!-- # projct 1 -->
    <tr onmouseout="ever_stop()" onmouseover="ever_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='tuber'>
					  <img src='images/tubeR.png' width=100%>
					</div>
          <img src='images/tubeR.png' width=100%>
        </div>
        <script type="text/javascript">
          function ever_start() {
            document.getElementById('tuber').style.opacity = "1";
          }
          function ever_stop() {
            document.getElementById('tuber').style.opacity = "0";
          }
          ever_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://github.com/amazon-science/tubelet-transformer">
			<span class="papertitle">Tuber: Tubelet transformer for video action detection</span>
        </a>
        <br>
				Jiaojiao Zhao, Yanyi Zhang, Xinyu Li, Hao Chen, Bing Shuai, Mingze Xu, <strong>Chunhui Liu</strong>, Kaustav Kundu, Yuanjun Xiong, Davide Modolo, Ivan Marsic, Cees GM Snoek, Joseph Tighe
				<br>
        <em>CVPR</em>, 2022
        <br>
        <a href="https://github.com/amazon-science/tubelet-transformer">Code</a>
        /
        <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhao_TubeR_Tubelet_Transformer_for_Video_Action_Detection_CVPR_2022_paper.pdf">Paper</a>
        <p></p>
        <p>
				The first SOTA transformer model on Action Detections, using learnable queries as tubelet proposals.
        </p>
      </td>
    </tr> 

<!-- # projct 2 -->
<tr onmouseout="ever_stop()" onmouseover="ever_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id='sfc'>
        <img src='images/sfc.png' width=100%>
      </div>
      <img src='images/sfc.png' width=100%>
    </div>
    <script type="text/javascript">
      function ever_start() {
        document.getElementById('sfc').style.opacity = "1";
      }
      function ever_stop() {
        document.getElementById('sfc').style.opacity = "0";
      }
      ever_stop()
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Selective_Feature_Compression_for_Efficient_Activity_Recognition_Inference_ICCV_2021_paper.pdf">
  <span class="papertitle">Selective Feature Compression for Efficient Activity Recognition Inference
  </span>
    </a>
    <br>
    <strong>Chunhui Liu</strong>, Xinyu Li, Hao Chen, Davide Modolo, Joseph Tighe
    <br>
    <em>ICCV</em>, 2021
    <br>
    <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Selective_Feature_Compression_for_Efficient_Activity_Recognition_Inference_ICCV_2021_paper.pdf">Paper</a>
    <p></p>
    <p>
    Utilizing transformers as spatial feature sampler, achieve 6x faster inference without performance drop.
    </p>
  </td>
</tr> 

<!-- # projct 3 -->
<tr onmouseout="ever_stop()" onmouseover="ever_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id='vidtr'>
        <img src='images/vidtr.png' width=100%>
      </div>
      <img src='images/vidtr.png' width=100%>
    </div>
    <script type="text/javascript">
      function ever_start() {
        document.getElementById('vidtr').style.opacity = "1";
      }
      function ever_stop() {
        document.getElementById('vidtr').style.opacity = "0";
      }
      ever_stop()
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_VidTr_Video_Transformer_Without_Convolutions_ICCV_2021_paper.pdf">
  <span class="papertitle">Vidtr: Video transformer without convolutions
  </span>
    </a>
    <br>
    Yanyi Zhang, Xinyu Li, <strong>Chunhui Liu</strong>, Bing Shuai, Yi Zhu, Biagio Brattoli, Hao Chen, Ivan Marsic, Joseph Tighe
    <br>
    <em>ICCV</em>, 2021
    <br>
    <a href="https://github.com/amazon-science/gluonmm/blob/main/src/transformers/models/vidtr/README.md">Code</a> / 
    <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_VidTr_Video_Transformer_Without_Convolutions_ICCV_2021_paper.pdf">Paper</a>
    <p></p>
    <p>
      One of the earliest works to use the transformer architecture for action recognition
    </p>
  </td>
</tr> 


<!-- # projct 4 -->
<tr onmouseout="ever_stop()" onmouseover="ever_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id='survey'>
        <img src='images/survey.png' width=100%>
      </div>
      <img src='images/survey.png' width=100%>
    </div>
    <script type="text/javascript">
      function ever_start() {
        document.getElementById('survey').style.opacity = "1";
      }
      function ever_stop() {
        document.getElementById('survey').style.opacity = "0";
      }
      ever_stop()
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://github.com/dmlc/gluon-cv/blob/master/scripts/action-recognition/ARXIV.md">
  <span class="papertitle">A comprehensive study of deep video action recognition
  </span>
    </a>
    <br>
    Yi Zhu, Xinyu Li, <strong>Chunhui Liu</strong>, Mohammadreza Zolfaghari, Yuanjun Xiong, Chongruo Wu, Zhi Zhang, Joseph Tighe, R Manmatha, Mu Li
    <br>
    <em>Arxiv</em>, 2021
    <br>
    <a href="https://cv.gluon.ai/model_zoo/action_recognition.html">Code</a> / 
    <a href="https://arxiv.org/abs/2012.06567">Paper</a> /
    <a href="https://bryanyzhu.github.io/videomodeling.github.io/"> Tutorial</a>
    <p></p>
    <p>
      We present a survey paper that summarizes 16 datasets and 200 existing papers on action understanding. Additionally, we provide tutorial workshops and a complete codebase to help newcomers join this field.
    </p>
  </td>
</tr> 


<!-- # projct 5 -->
<tr onmouseout="ever_stop()" onmouseover="ever_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id='pkummd'>
        <img src='images/pkummd.png' width=100%>
      </div>
      <img src='images/pkummd.png' width=100%>
    </div>
    <script type="text/javascript">
      function ever_start() {
        document.getElementById('pkummd').style.opacity = "1";
      }
      function ever_stop() {
        document.getElementById('pkummd').style.opacity = "0";
      }
      ever_stop()
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://www.icst.pku.edu.cn/struct/Projects/PKUMMD.html">
  <span class="papertitle">PKU-MMD: A Large Scale Benchmark for Continuous Multi-Modal Human Action Understanding
  </span>
    </a>
    <br>
    <strong>Chunhui Liu</strong>, Yueyu Hu, Yanghao Li, Sijie Song, Jiaying Liu
    <br>
    <em>ACM MM Workshop</em>, 2017
    <br>
    <a href="https://github.com/ECHO960/PKU-MMD">Code</a> / 
    <a href="https://arxiv.org/abs/1703.07475">Paper</a> /
    <a href="https://www.icst.pku.edu.cn/struct/Projects/PKUMMD.html/"> Project Page</a> /
    <a href="https://drive.google.com/drive/folders/0B20a4UzO-OyMUVpHaWdGMFY1VDQ?resourcekey=0-ZHRrEctA8yR2EtTgLnRjwg"> Dataset</a>
    <p></p>
    <p>
      We present a survey paper that summarizes 16 datasets and 200 existing papers on action understanding. Additionally, we provide tutorial workshops and a complete codebase to help newcomers join this field.
    </p>
  </td>
</tr> 

          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  This template is forked from <a href="https://github.com/jonbarron/jonbarron_website">source code</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
